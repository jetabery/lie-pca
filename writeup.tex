\documentclass[reqno]{amsart}
\usepackage{geometry}
\usepackage{dsfont}
\usepackage{amssymb}

\renewcommand{\epsilon}{\varepsilon}

\DeclareMathOperator{\diag}{diag}

\newtheorem{conj}{Conjecture}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}

\title{Recovering Lie Groups from Orbits}
\author{Julius Tabery}
\date{September 2024}

\begin{document}

\maketitle

\section{Best-Case Scenario}
Suppose $Q \in U(d)$ and $a \in \mathbb{Z}^d$.
Assume $a$ is not the zero vector.
Then the Lie group $G = \{Qe^{\diag(2\pi i a t)} Q^{-1} : t \in [0,1)\}$ is a representation of the 1-torus $\mathbb{T}$.
We identify $\mathbb{T}$ with the interval $[0,1)$ with addition modulo 1.
Denote $G(t) = Qe^{\diag(2\pi i a t)} Q^{-1}$.
Let $x \in \mathbb{C}^d$ and assume the map $t \mapsto G(t) x$ is injective for $t \in \mathbb{T} \cong \mathbb{R} / \mathbb{Z}$.
(TODO: prove this assumption holds for generic $x$).
Consider the points $x_1, \dots, x_n \in \mathbb{C}^d$, where  $x_j = G(\frac{j}{n})x$ for $j \in [n]$.


If, for each $j \in [n]$, we are given the pair $(j, x_j)$, (that is, the order of the points is known), we can recover the simplest Lie group which generates this orbit.
The recovery process is as follows:

For every $b \in \mathbb{Z}$, let 
\[
y_b = \sum_{j=1}^n e^{-2\pi ibj/n}x_j.
\]
Then
\begin{align*}
    y_b
    &=
    \sum_{j=1}^n e^{-2\pi ibj/n} Q e^{\diag(2\pi iaj/n)}Q^{-1}x
    \\&=
    Q\diag\left(
    \sum_{j=1}^n e^{\frac{2\pi i j}{n} (a - b\mathds{1})}
    \right)Q^{-1}x
    \\&=
    Q \diag\left( n\, \mathds{1}_{a_k= b} \right)Q^{-1}x
    \\&=
    n P_b x,
\end{align*}
where $P_b$ is the orthogonal projector onto the subrepresentation of $G$ corresponding to $b$ (note that $P_b=0$ if $b \notin a$).
Now, since we only have one orbit, we may assume that all nontrivial subrepresentations are 1-dimensional.
So, if $y_b \neq 0$, then $P_b = \frac{y_b y_b^*}{\|y_b\|^2}$.
Finally, for $t \in [0,1]$, let 
\[
    H(t)
    =
    I +\sum_{\substack{b \in \mathbb{Z} \\ y_b \neq 0}} 
    \frac{y_b y_b^*}{\|y_b\|^2}(e^{2\pi ibt} -1).
\]
Then
\begin{align*}
    H(t)x
    &=
    \left[\sum_{b \in a} 
    P_b e^{2\pi ibt}
    + \left( I - \sum_{b \in a}
    P_b \right)\right]x
    \\&= G(t)x,
\end{align*}
and so the Lie group $H=\{H(t): t \in [0,1)\}$ is the simplest lie group such that $H \cdot x = G \cdot x$.
(i.e. $H$ is the simplest Lie group required to reconstruct the given dataset.)
If all nontrivial subrepresentations of $G$ are 1-dimensional, (i.e. if $a$ has no repeats) then $H=G$.
(TODO: prove that $H$ is actually a Lie group.)

TODO: Re-formulate in terms of a discrete Fourier transform. (might also simplify some proofs)

\section{What if the Dataset is Reordered/Unordered?}

Let $D_n$ denote the dihedral group as a subgroup of the symmetric group $S_n$.
$D_n$ is generated by rotations and reflections.
\begin{thm}\label{dihedral-invariance}
    Suppose that $H_1$ is calculated as described above from the ordered dataset $x_1, \dots, x_n$. 
    Now let $\sigma \in D_n$ and suppose that $H_2$ is calculated as described above from the ordered dataset $x_{\sigma(1)}, \dots, x_{\sigma(n)}$.
    Then $H_1=H_2$.
\end{thm}
\begin{proof}
    Every member of $D_n$ can be expressed as the composition of a rotation and a reflection (that is, $D_n \cong C_2 \ltimes C_n$).
    So, it suffices to prove that the desired property holds for these two kinds of permutations.

    First, assume $\sigma \in D_n$ is a rotation.
    Then there is a nonnegative integer $k$ such that for $j \in [n]$, we have $\sigma(j) = \begin{cases}
        j+k &,\ j+k \leq n \\
        j+k-n &,\ j+k > n
    \end{cases}$
    and
    $\sigma^{-1}(j) = \begin{cases}
        j-k &,\ j-k \geq 1 \\
        j-k+n &,\ j-k <1
    \end{cases}$.
    Recall that, above, $H$ is defined in terms of some $y_b$, for $b \in \mathbb{Z}$.
    Let $y_b^{(1)}$ denote the $y_b$ which determine $H_1$ and let $y_b^{(2)}$ denote the $y_b$ which determine $H_2$.
    Then for $b \in \mathbb{Z}$,
    \begin{align*}
    y_b^{(2)}
    =
    \sum_{j=1}^n e^{\frac{-2\pi ibj}{n}}x_{\sigma(j)}
    =
    \sum_{j=1}^n e^{\frac{-2\pi ib \sigma^{-1}(j)}{n}} x_j
    =
    \sum_{j=1}^n e^{\frac{-2\pi ib (j-k)}{n}} x_j
    =
    e^{\frac{2\pi ibk}{n}}
    \sum_{j=1}^n e^{\frac{-2\pi ib j}{n}} x_j
    =
    e^{\frac{2\pi ibk}{n}}y_b^{(1)}.
    \end{align*}
    Thus, in this case, for $t \in [0,1]$ we find
    \[
    h_2(t) 
    = 
    I +\sum_{\substack{b \in \mathbb{Z} \\ y^{(2)}_b \neq 0}} 
    \frac{y^{(2)}_b (y^{(2)}_b)^*}{\|y^{(2)}_b\|^2}(e^{2\pi ibt} -1)
    =
    I +\sum_{\substack{b \in \mathbb{Z} \\ y^{(1)}_b \neq 0}} 
    \frac{y^{(1)}_b (y^{(1)}_b)^*}{\|y^{(1)}_b\|^2}(e^{2\pi ibt} -1)
    =
    h_1(t),
    \]
    so, $H_1 = \{h_1(t): t \in [0,1]\} = \{h_2(t) : t \in [0,1]\} = H_2$.
    This proves the first case.

    Now, assume $\sigma \in X_n$ is a reflection.
    Without loss of generality, assume that for $j \in [n],$ we have $\sigma(j) = n-j+1$.
    Notice that $\sigma$ is its own inverse.
    It follows that for $b \in \mathbb{Z}$,
    \begin{align*}
    y_b^{(2)}
    =
    \sum_{j=1}^n e^{\frac{-2\pi ibj}{n}}x_{\sigma(j)}
    =
    \sum_{j=1}^n e^{\frac{-2\pi ib \sigma^{-1}(j)}{n}} x_j
    =
    \sum_{j=1}^n e^{\frac{-2\pi ib (n-j+1)}{n}} x_j
    &=
    e^{\frac{-2\pi ib}{n}}\sum_{j=1}^n e^{\frac{-2\pi i(-b)j}{n}}x_j
    \\&=
    e^{\frac{-2\pi ib}{n}} y_{-b}^{(1)}.
    \end{align*}
    Thus, in this case, for $t \in [0,1]$ we find
    \begin{align*}
    h_2(t) 
    = 
    I +\sum_{\substack{b \in \mathbb{Z} \\ y^{(2)}_b \neq 0}} 
    \frac{y^{(2)}_b (y^{(2)}_b)^*}{\|y^{(2)}_b\|^2}(e^{2\pi ibt} -1)
    &=
    I +\sum_{\substack{b \in \mathbb{Z} \\ y^{(1)}_{-b} \neq 0}} 
    \frac{y^{(1)}_{-b} (y^{(1)}_{-b})^*}{\|y^{(1)}_{-b}\|^2}(e^{2\pi ibt} -1)
    \\&=
    I +\sum_{\substack{b \in \mathbb{Z} \\ y^{(1)}_{b} \neq 0}} 
    \frac{y^{(1)}_{b} (y^{(1)}_{b})^*}{\|y^{(1)}_{b}\|^2}(e^{2\pi i(-b)t} -1)
    \\&=
    h_1(1-t),
    \end{align*}
    so $H_1 = \{h_1(t): t \in [0,1]\} = \{h_2(t) : t \in [0,1]\} = H_2$.
    This proves the second case.
\end{proof}


\begin{lemma}
    Suppose $f:\mathbb{T} \to \mathbb{R}$ is differentiable, $f'(0) \neq 0$, and $f^{-1}(\{0\})=\{0\}$.
    Then
    \[
    \lim_{n \to \infty}
    \int_{-n/2}^{n/2}\left|
    e^{
    -\frac{n^2}{f'(0)^2} f(t/n)^2}
    - e^{-t^2}\right|dt
    =0,
    \]
    where we identify $\mathbb{T}$ with $\left[-\frac12,\frac12\right]$ for the domain of $f$.
\end{lemma}
\begin{proof}
    First, assume $f'(0) = 1$.
    Then
    \[
        \int_{-n/2}^{n/2}\left|
        e^{
        -n^2 f(t/n)^2}
        - e^{-t^2}\right|dt
        =
        \underbrace{
        \int_{-n^{1/4}}^{n^{1/4}}\left|
        e^{-n^2 f(t/n)^2}
        - e^{-t^2}\right|dt}_A
        % \\&\qquad \qquad
        +\underbrace{
        \int_{\left[-\frac{n}{2}, \frac{n}{2}\right]\setminus
        \left[-n^{1/4}, n^{1/4}\right]}
        \left|
        e^{-n^2 f(t/n)^2}
        - e^{-t^2}\right|dt}_B
    \]
    Now
    \begin{align*}
        B
        &=
        \int_{|t| \geq n^{1/4}}\left|
        \exp\left(
        -n^2 f(t/n)^2 \right)
        - \exp\left(-t^2\right)\right|dt
        \\&\leq
        \left(n - 2n^{1/4}\right)\sup_{|t| \geq n^{1/4}}
        \left|e^{-n^2f(t/n)^2} - e^{-t^2}\right|
        \\&\leq
        \left(n - 2n^{1/4}\right)\left(
        \sup_{|t| \geq n^{1/4}}
        e^{-n^2f(t/n)^2}
        + \sup_{|t| \geq n^{1/4}}
        e^{-t^2}\right)
    \end{align*}
    Given the assumptions about $f$, both suprema tend to 0 exponentially as $n \to \infty$, so $B \to 0$ as $n \to \infty$.
    
    Let $g(t) = f(t) - t$, so $g(t) = O(t^2)$.
    Then
    \begin{align*}
        A
        &=
        \int_{-n^{1/4}}^{n^{1/4}}\left|
        \exp\left(
        -n^2 f(t/n)^2 \right)
        - \exp\left(-t^2\right)\right|dt
        \\&=
        \int_{-n^{1/4}}^{n^{1/4}}\left|
        \exp\left(-n^2\left(
        \frac{t}{n} + g(t/n)\right)^2\right)
        - e^{-t^2}\right|dt
        \\&=
        \int_{-n^{1/4}}^{n^{1/4}}\left|
        \exp\left(-t^2-2ntg\left(\frac{t}{n}\right)
        -n^2g\left(\frac{t}{n}\right)^2\right)
        - e^{-t^2}\right|dt
        \\&=
        \int_{-n^{1/4}}^{n^{1/4}}
        e^{-t^2}\left|
        \exp\left(-2ntg\left(\frac{t}{n}\right)
        -n^2g\left(\frac{t}{n}\right)^2\right)
        - 1\right|dt
        \\&\leq
        \int_{-n^{1/4}}^{n^{1/4}}
        e^{-t^2}\, dt \cdot
        \sup_{|t| \leq n^{1/4}}\left|
        \exp\left(-2ntg\left(\frac{t}{n}\right)
        -n^2g\left(\frac{t}{n}\right)^2\right)
        - 1\right|
        \\&\leq
        \sqrt{\pi} \cdot 
        \sup_{|t| \leq n^{1/4}}\left|
        \exp\left(-2ntg\left(\frac{t}{n}\right)
        -n^2g\left(\frac{t}{n}\right)^2\right)
        - 1\right|
        \\&=
        \sqrt{\pi} \cdot 
        \sup_{|t| \leq n^{1/4}}\left|
        e^{h(t)}
        - 1\right|,
        \qquad \text{ where} \quad
        h(t) = -2ntg\left(\frac{t}{n}\right)
        -n^2g\left(\frac{t}{n}\right)^2
    \end{align*}
    Let $a = \frac12 f''(0)$.
    Then $g(t) = at^2 + O(t^3)$.
    Thus, for $|t| \leq n^{1/4}$,
    \begin{align*}
        |h(t)|
        &=
        \left| -2ntg\left(\frac{t}{n}\right)
        -n^2g\left(\frac{t}{n}\right)^2 \right|
        \\&\leq
        2n^{5/4}\left|
        a\left(\frac{t}{n}\right)^2
        + O\left(\frac{t}{n}\right)^3
        \right|
        + n^2 \left|
        a\left(\frac{t}{n}\right)^2
        + O\left(\frac{t}{n}\right)^3
        \right|
        \\&=
        2n^{5/4}\left|
        O\left((n^{-3/2}\right)
        \right|
        + n^2 \left|
        O\left(n^{-3}\right)
        \right|
        \\&=
        O\left(n^{-1/4}\right)
    \end{align*}
    Thus, $A \to 0$ as $n \to \infty$.
    
    This proves the case $f'(0)=1$.
    The general case follows from observing that 
    \[
    \frac{d}{dt}
    \frac{f(t)}{f'(0)} = 1.
    \]
    TODO: Clean this proof up a bit.
\end{proof}


\begin{lemma}
    Suppose the ordered dataset $x_1, \dots, x_n$ is as described above and define $K \in \mathbb{R}^{n \times n}$ by $K_{ij} = \exp\left(\frac{-\|x_i - x_j\|^2}{2\epsilon^2}\right)$, where $\epsilon = \min\{\|x_i - x_j\| : i \neq j\}$.
    Then for sufficiently large $n$, the eigenvectors corresponding to the second and third largest eigenvectors of $K$ will be orthogonal sinusoids (eg. sin, cos) of frequency 1.
\end{lemma}
\begin{proof}
    
    % We will write the periodization of the Gaussian as
    % \[
    % g(t) = \sum_{k \in \mathbb{Z}}
    % e^{-\pi (t+k)^2}
    % \]
    
    Let $\phi:\mathbb{T}\to\mathbb{C}^d$ be given by $\phi(t) = G(t)x$.
    Define $f(t) = \left\|\phi(0)-\phi(t)\right\|$.
    After a change of variables, the previous lemma reveals that
    \[
    \lim_{\epsilon\to0}
    \frac{1}{\epsilon}\int_{-1/2}^{1/2}
    \left|e^{\frac{f(t)^2}{f'(0)^2\epsilon^2}} - e^{\frac{-t^2}{\epsilon^2}}\right|dt
    =0.
    \]
    We also observe that
    \[
    \lim_{\epsilon\to0}
    \frac{1}{\epsilon}
    \int_{-1/2}^{1/2}e^{-\frac{t^2}{\epsilon^2}}dt
    =
    \lim_{\epsilon\to0}
    \int_{-\frac{1}{2\epsilon}}^{\frac{1}{2\epsilon}}
    e^{t^2}dt
    =\sqrt{\pi},
    \]
    and so the lemma implies that $\exp\left(\frac{f(t)^2}{f'(0)^2\epsilon^2}\right)$ converges to the Gaussian in a normalized sense.
    Also, we have shown $L^1$ convergence, but since the functions involved are bounded, then by H\"older's inequality, the convergence also holds in $L^2$.
    Since the Fourier transform is a unitary operator on $L^2$ and the Fourier transform of a Gaussian is another Gaussian, then the Fourier transform of $\exp\left(\frac{f(t)^2}{f'(0)^2\epsilon^2}\right)$ converges to a Gaussian in a normalized sense.
    
    TODO: argue that the periodization of the Gaussian is basically the Gaussian for small $\epsilon$
    
    TODO: argue that the functions are well-behaved enough that this convergence holds pointwise
    
    TODO argue that properties carry over when we discretize
    
    Since the eigenvalues of a circulant matrix are simply the entries of the Fourier transform of its first row, and since the Gaussian is centrally concentrated, then this completes the proof.
\end{proof}

\pagebreak
\begin{thm}\label{sin-cos-evecs}
    Suppose the ordered dataset $x_1, \dots, x_n$ is as described above. 
    Let $\sigma \in S_n$ and let $\epsilon > 0$.    
    Define $K \in \mathbb{R}^{n \times n}$ by $K_{ij} = \exp\left( \frac{-\|x_{\sigma(i)} - x_{\sigma(j)} \|^2}{2\epsilon^2}\right)$.
    Let $u, v$, be the eigenvectors corresponding to the second and third largest eigenvalues of $K$, respectively.
    Let $\gamma \in S_n$ be the permutation which sorts $(\alpha(u_1, v_1), \dots, \alpha(u_n, v_n))$ into ascending order, where $\alpha(a,b)$ is defined as the angle from the origin described by the point $(a,b) \in \mathbb{R}^2$.
    Then there exists an appropriate choice of $\epsilon$ such that $\gamma \circ \sigma \in D_n$.
\end{thm}
\begin{proof}
    Firstly, if $S$ is a permutation matrix, then for an eigenvector $v$ with eigenvalue $\lambda$, we have
    \[
    SKS^T (Sv) = SKv = \lambda (Sv),
    \]
    and so $SKS^T$ has the same eigenvalues as $K$, with eigenvectors whose entries have been permuted according to $S$.
    So, entrywise correspondence of the eigenvectors is preserved under arbitrary permutations, hence the "unwrapping" procedure is invariant under these permutations.
    Thus, it suffices to show that the "unwrapping" procedure, when performed on the original dataset, will yield an element of $D_n$.
    But since these vectors correspond to orthogonal Fourier modes of frequency 1, then they will be sinusoids with phases offset by $\frac{\pi}{2}$.
    The result follows.
    
    TODO: rewrite with less hand-waving
\end{proof}

Theorems \ref{dihedral-invariance} and \ref{sin-cos-evecs} combine to yield a procedure similar to the one described in section 1, but without needing to know the ordering of the dataset.

\begin{itemize}
    
    \item It would also be nice to prove that, with noise, this happens with high probability.
    
    \item Maybe we could relax to having that $d(\gamma \circ \sigma, D_n)$ is small for some metric $d$.
\end{itemize}

\section{What if the points are not evenly spaced?}
Let $t_1, \dots, t_n$ be sampled uniformly and independently from $\mathbb{T}$.
We will now consider the dataset $x_{t_1}, \dots, x_{t_n}$.

Goal: We can construct the matrix $K$ as before, perform Sinkhorn's algorithm, and then proceed as in the evenly-spaced case.

\section{What if we add noise?}



\end{document}
